{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = '/home/username/authorship_attribution\n",
    "dir = '/home/alissia/authorship_attribution'\n",
    "flist = [join(dir+'/dataset', f) for f in listdir(dir) if isfile(join(dir+'/dataset', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenfiles = []\n",
    "for file in flist: \n",
    "    df = pd.read_csv(join(dir+'/dataset', file), sep=',', encoding='utf-8')\n",
    "    lenfiles.append(len(df))\n",
    "files = sorted(list(zip(lenfiles, flist)), reverse=True)[:50] \n",
    "# dataset contains more than 50 files but for experiments it is more convenient to use smaller dataset\n",
    "# 1 author = 1 file of 3200 tweets (the limit of API Twitter), one tweet per line\n",
    "# 2 columns: username and text with delimiter ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(dir,'kaomodji.txt'), 'r') as f:\n",
    "    kaomodji = [k.strip() for k in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization -- optional\n",
    "def lemm(word):\n",
    "    return morph.parse(word.lower())[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):  \n",
    "    tweet = re.sub(\"@\\w+\", \"_mention_\", tweet)\n",
    "    tweet = re.sub(\"https?:\\/\\/[^\\s]*\", \"_URL_\", tweet)\n",
    "    tweet = re.sub(\"#[^\\s]*\", \"_hashtag_\", tweet)\n",
    "    tweet = re.sub(\":.*:\", \"_emodji_\", emoji.demojize(tweet))\n",
    "    tweet = tweet + \" \".join(['_kaomodji_' for i in tweet.split(' ') if i in kaomodji])\n",
    "    tweet = re.sub(\":-?\\)|:-?\\(|:-?O|:-?Ъ|\\)-?:|\\(-?:|=\\)|=\\(|\\)=|\\(=|;-?\\)|;-?\\(\", \"_emoticon_\", tweet)\n",
    "    tweet = re.sub(\"-|−|–|--\", \"—\", tweet)\n",
    "    tweet = re.sub(r\"([\\w/'+$\\s-]+|[^\\w/'+$\\s-]+)\\s*\", r\"\\1 \", tweet)\n",
    "    return \" \".join([lemm(word) for word in tweet.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the length of tweet which is is 1-270 chars (from 2000-3000+ lines to 1000)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(files)): \n",
    "    tweets = []\n",
    "    df = pd.read_csv(join(dir+'/dataset', files[i][1]), sep=',', encoding='utf-8')\n",
    "    y += df['Username'].tolist()[:1000]\n",
    "    # exclude tweets shorter than 3 words\n",
    "    data = [preprocess(tweet) for tweet in df['Tweets'].tolist()]\n",
    "    # the value is not more than 3200 (the limit of API Twitter)\n",
    "    if len(data) > 1999: \n",
    "        for i in range(0,2000,2):\n",
    "            tweets.append(data[i] + ' ' + data[i+1])\n",
    "        res = len(data) - 2000\n",
    "    # the authors were selected to exclude the accounts with less than 1000 tweets\n",
    "    else:  \n",
    "        tweets = data[:1000]\n",
    "        res = len(data) - 1000\n",
    "    data = data[-res:]\n",
    "    if res > 1000:\n",
    "        for i in range(1000):\n",
    "            tweets[i] = tweets[i] + ' ' + data[i]\n",
    "        res = res - 1000\n",
    "        data = data[-res:]\n",
    "        for i in range(res):\n",
    "            tweets[i] = tweets[i] + ' ' + data[i]\n",
    "    else:\n",
    "        for i in range(res):\n",
    "            tweets[i] = tweets[i] + ' ' + data[i]\n",
    "\n",
    "    x += tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(y))==len(x)\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(x,y))\n",
    "df = pd.DataFrame(data, columns = ['text', 'author'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['text'], df['author']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(X_train))) # 75%\n",
    "print(len(list(X_test))) # 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = sorted(set(y_test))\n",
    "print(len(authors)) # 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "with open(join(dir, 'ebd_train.txt'), 'w', encoding='utf-8') as f:\n",
    "    for line in list(X_train):\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "mixed = sorted(list(zip(y_test, X_test)))\n",
    "for author in authors:\n",
    "    with open(join(dir+'ebd_data', author+'.txt'), 'w', encoding='utf-8') as f:\n",
    "        for i in range(len(mixed)):\n",
    "            if mixed[i][0] == author:            \n",
    "                f.write(str(mixed[i][1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
